{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz7LMjsQGBtr",
        "outputId": "e0261555-748e-4918-9714-1fd790dbe15e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1))       # To flatten the tensors\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1))       # To flatten the tensors\n",
        "])\n",
        "\n",
        "# Loading the complete train MNIST dataset.\n",
        "full_train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "\n",
        "# Splitting the train dataset into validation.\n",
        "\n",
        "train_size = int(0.9*len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "\n",
        "train_subset, val_subset = torch.utils.data.random_split(\n",
        "    full_train_dataset, [train_size, val_size],\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "\n",
        "# Creating a class to apply transforms separatly to subsets\n",
        "\n",
        "class ApplyTransform(torch.utils.data.Dataset):\n",
        "  def __init__(self, subset, transform=None):\n",
        "    self.subset = subset\n",
        "    self.transform=transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x, y = self.subset[index]\n",
        "    if self.transform:\n",
        "      x = self.transform(x)\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.subset)\n",
        "\n",
        "train_dataset = ApplyTransform(train_subset, train_transform)\n",
        "val_dataset = ApplyTransform(val_subset, val_test_transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=val_test_transform\n",
        ")\n",
        "\n",
        "\n",
        "# Creating the DataLoaders\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfaTkLE7GZTP",
        "outputId": "71f5f6bf-d5e6-4245-81e6-7ff71d72c914"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 58.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.14MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 13.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.36MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
        "    super(MLP, self).__init__()\n",
        "    layers = []\n",
        "    prev_size = input_size\n",
        "    for size in hidden_sizes:\n",
        "      layers.extend([\n",
        "          nn.Linear(prev_size, size),\n",
        "          nn.BatchNorm1d(size),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(dropout_prob)\n",
        "      ])\n",
        "      prev_size = size\n",
        "    layers.append(nn.Linear(prev_size, output_size))\n",
        "    self.net = nn.Sequential(*layers)       # Now can run all layers in one line of code as in forward method.\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        ""
      ],
      "metadata": {
        "id": "IrH6ONpDK22e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "\n",
        "input_size = 784\n",
        "hidden_sizes = [256, 128]\n",
        "output_size = 10\n",
        "dropout_prob = 0.3\n",
        "lr = 0.001\n",
        "\n",
        "\n",
        "''' model.to(device) moves the model to the \"device\". Its necessary that both the\n",
        " data and the model are in the same device otherwise we'll get a runtime error'''\n",
        "\n",
        "\n",
        "model = MLP(input_size, hidden_sizes, output_size, dropout_prob).to(device)"
      ],
      "metadata": {
        "id": "7Mnn-NyDM31o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "''' Kaiming/He distribution function: Fills the tensor with the distribution U(-bound, bound) where\n",
        "    bound = gain*sqrt(3/fan_mode)'''\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "''' The \"apply\" method in nn.Module class.Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.'''\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YuPCajcNcKm",
        "outputId": "d91b7aa7-96f3-4ab1-aca7-2c668247e03d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (net): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.3, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=20):\n",
        "  best_val_acc = 0.0\n",
        "  history = {\n",
        "      'train_loss' : [],\n",
        "      'train_acc' : [],\n",
        "      'val_loss' : [],\n",
        "      'val_acc' : []\n",
        "  }\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "\n",
        "    # Sets the self.training = True and moves the model into training mode.\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)     # Move the data into the \"device(GPU)\"\n",
        "      optimizer.zero_grad()               # Initializes the gradients with zero values. Otherwise gradients will add up after few epoches.\n",
        "      outputs = model(inputs)             # Callable through __call__ method in nn.Module class.\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()                      # Calculates gradient d(loss)/dx for every parameter x.\n",
        "      optimizer.step()                     # Update step\n",
        "\n",
        "      running_loss+= loss.item()          # Tensor to float\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss/len(train_loader)\n",
        "    train_acc = correct/total\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "\n",
        "    #Validation Phase\n",
        "    model.eval()        # Puts the model in evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_correct, val_total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct/val_total\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Saving the best model\n",
        "    if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}: '\n",
        "              f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} '\n",
        "              f'Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "  # Load best model for final evaluation\n",
        "  model.load_state_dict(torch.load('best_model.pth'))\n",
        "  return model, history\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train model\n",
        "model = MLP(input_size, hidden_sizes, output_size, dropout_prob).to(device)\n",
        "model.apply(init_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "trained_model, history = train_model(model, criterion, optimizer, num_epochs=20)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pLQBDc3P_dM",
        "outputId": "ad5f5ecf-b6e6-4071-8db8-4afa27e20f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20: Train Loss: 0.5681 | Val Loss: 0.1755 Train Acc: 0.8215 | Val Acc: 0.9480\n",
            "Epoch 2/20: Train Loss: 0.3107 | Val Loss: 0.1266 Train Acc: 0.9030 | Val Acc: 0.9637\n",
            "Epoch 3/20: Train Loss: 0.2552 | Val Loss: 0.1015 Train Acc: 0.9217 | Val Acc: 0.9695\n",
            "Epoch 4/20: Train Loss: 0.2241 | Val Loss: 0.0901 Train Acc: 0.9300 | Val Acc: 0.9718\n",
            "Epoch 5/20: Train Loss: 0.2034 | Val Loss: 0.0821 Train Acc: 0.9374 | Val Acc: 0.9750\n",
            "Epoch 6/20: Train Loss: 0.1896 | Val Loss: 0.0763 Train Acc: 0.9409 | Val Acc: 0.9765\n",
            "Epoch 7/20: Train Loss: 0.1757 | Val Loss: 0.0718 Train Acc: 0.9445 | Val Acc: 0.9785\n",
            "Epoch 8/20: Train Loss: 0.1646 | Val Loss: 0.0686 Train Acc: 0.9495 | Val Acc: 0.9800\n",
            "Epoch 9/20: Train Loss: 0.1586 | Val Loss: 0.0641 Train Acc: 0.9497 | Val Acc: 0.9805\n",
            "Epoch 10/20: Train Loss: 0.1509 | Val Loss: 0.0618 Train Acc: 0.9538 | Val Acc: 0.9827\n",
            "Epoch 11/20: Train Loss: 0.1468 | Val Loss: 0.0587 Train Acc: 0.9548 | Val Acc: 0.9820\n",
            "Epoch 12/20: Train Loss: 0.1482 | Val Loss: 0.0567 Train Acc: 0.9540 | Val Acc: 0.9828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history after training\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "CM5tp5bNZIUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyperparams(param_grid):\n",
        "    best_params = {}\n",
        "    best_acc = 0\n",
        "\n",
        "    for lr in param_grid['lr']:\n",
        "        for dropout in param_grid['dropout']:\n",
        "            print(f'Trying lr={lr}, dropout={dropout}')\n",
        "            model = MLP(input_size, hidden_sizes, output_size, dropout).to(device)\n",
        "            model.apply(init_weights)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Train for reduced epochs for quick evaluation\n",
        "            model, _ = train_model(model, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_acc = history['val_acc'][-1]\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                best_params = {'lr': lr, 'dropout': dropout}\n",
        "\n",
        "    print(f'Best Params: {best_params} | Best Val Acc: {best_acc:.4f}')\n",
        "    return best_params\n",
        "\n",
        "# Example usage\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.0005, 0.0001],\n",
        "    'dropout': [0.3, 0.5, 0.7]\n",
        "}\n",
        "\n",
        "best_params = tune_hyperparams(param_grid)"
      ],
      "metadata": {
        "id": "VKbNTCybA6uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Combine original training and validation sets\n",
        "full_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
        "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 2. Initialize final model with best parameters\n",
        "final_model = MLP(\n",
        "    input_size=input_size,\n",
        "    hidden_sizes=hidden_sizes,\n",
        "    output_size=output_size,\n",
        "    dropout_prob=best_params['dropout']\n",
        ").to(device)\n",
        "final_model.apply(init_weights)\n",
        "\n",
        "# 3. Use optimized learning rate\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
        "\n",
        "# 4. Train on full dataset (original train + val)\n",
        "final_model, final_history = train_model(\n",
        "    final_model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs=20  # Use original epoch count\n",
        ")"
      ],
      "metadata": {
        "id": "lgsocli_ZvvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def final_evaluation(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    print(f\"\\nFinal Test Results:\")\n",
        "    print(f\"Loss: {test_loss:.4f}\")\n",
        "    print(f\"Accuracy: {test_acc:.4f}\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Run evaluation\n",
        "test_loss, test_acc = final_evaluation(final_model, test_loader)"
      ],
      "metadata": {
        "id": "xgDq4YESZ8Iv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}